{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook we created TFrecords from the bible. In the notebook we'll take a look at them and then build a model that predicts the Book of the bible a verse came from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from preppy import Preppy\n",
    "class BibPreppy(Preppy):\n",
    "    '''\n",
    "    We'll slightly extend to way we right tfrecords to store the id of the book it came from\n",
    "    '''\n",
    "    def sequence_to_tf_example(self,sequence,book_id):\n",
    "        id_list = self.sentance_to_id_list(sequence)\n",
    "        ex = tf.train.SequenceExample()\n",
    "        # A non-sequential feature of our example\n",
    "        sequence_length = len(sequence)\n",
    "        ex.context.feature[\"length\"].int64_list.value.append(sequence_length)\n",
    "        ex.context.feature[\"book_id\"].int64_list.value.append(book_id)\n",
    "        # Feature lists for the two sequential features of our example\n",
    "        fl_tokens = ex.feature_lists.feature_list[\"tokens\"]\n",
    "\n",
    "        for token in id_list:\n",
    "            fl_tokens.feature.add().int64_list.value.append(token)\n",
    "\n",
    "        return ex\n",
    "    @staticmethod\n",
    "    def parse(ex):\n",
    "        '''\n",
    "        Explain to TF how to go froma  serialized example back to tensors\n",
    "        :param ex:\n",
    "        :return:\n",
    "        '''\n",
    "        context_features = {\n",
    "            \"length\": tf.FixedLenFeature([], dtype=tf.int64),\n",
    "            \"book_id\": tf.FixedLenFeature([], dtype=tf.int64)\n",
    "        }\n",
    "        sequence_features = {\n",
    "            \"tokens\": tf.FixedLenSequenceFeature([], dtype=tf.int64),\n",
    "        }\n",
    "\n",
    "        # Parse the example (returns a dictionary of tensors)\n",
    "        context_parsed, sequence_parsed = tf.parse_single_sequence_example(\n",
    "            serialized=ex,\n",
    "            context_features=context_features,\n",
    "            sequence_features=sequence_features\n",
    "        )\n",
    "        return {\"seq\":sequence_parsed[\"tokens\"], \"length\": context_parsed[\"length\"], \n",
    "                \"book_id\": context_parsed[\"book_id\"]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a dataset by reading the train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "dataset = tf.data.TFRecordDataset(['./train.tfrecord']).map(BibPreppy.parse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_item = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess =tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'book_id': 3,\n",
       " 'length': 59,\n",
       " 'seq': array([25, 14, 13,  5, 11,  8,  7,  5, 27, 28, 29, 30,  5, 10,  6,  9, 36,\n",
       "         7,  5, 16, 14, 11,  3,  5, 41,  3, 10,  7, 10,  5,  9, 14, 13,  5,\n",
       "        16, 14, 11,  3,  5, 25,  9,  4,  3, 14, 15,  5, 10,  9, 34, 18, 14,\n",
       "        22, 15,  5, 62, 64, 39, 65, 62])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(next_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'book_id': TensorShape([]),\n",
       " 'length': TensorShape([]),\n",
       " 'seq': TensorShape([Dimension(None)])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def expand(x):\n",
    "    x['length'] = tf.expand_dims(tf.convert_to_tensor(x['length']),0)\n",
    "    x['book_id'] = tf.expand_dims(tf.convert_to_tensor(x['book_id']),0)\n",
    "    return x\n",
    "def deflate(x):\n",
    "    x['length'] = tf.squeeze(x['length'])\n",
    "    x['book_id'] = tf.squeeze(x['book_id'])\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_iter = dataset.map(expand).padded_batch(128,padded_shapes={\n",
    "    \"book_id\":1,\n",
    "    \"length\":1,\n",
    "    \"seq\":tf.TensorShape([None])\n",
    "}).map(deflate)\n",
    "next_item = batch_iter.repeat().make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'book_id': array([  3,   0, 214,  10,   4,   2,  17, 188, 194,   0,   1, 188, 201,\n",
       "          3,   3,   5,   5,   6, 214,   3,   3, 192,   0, 192, 190,   3,\n",
       "          8,  14, 170,  10, 214,  12, 197,   0, 180, 190, 189,   3, 173,\n",
       "          4, 171,   4, 195,   3, 173, 210,  10, 174, 173, 171, 168,   8,\n",
       "          3, 214, 167,   8,   0, 191, 206,  13, 180, 189, 175,   2, 167,\n",
       "        175, 188,   5,  10, 170, 185,   4,  10,   4,   6, 198, 194,   8,\n",
       "          1,   6,  14, 167,  10, 170, 191, 173,  12,   9,   1, 189,   3,\n",
       "          1, 167,  10, 189, 180,  12,   4, 171,   8, 190, 206,   4,   4,\n",
       "          6, 191,  11, 189, 174, 190,  14,   6,   8,   2,  11, 190, 170,\n",
       "        203, 191, 191,   0, 171, 193, 192, 198,   1,   9,  10]),\n",
       " 'length': array([59, 62, 64, 65, 30, 64, 64, 28, 61, 23, 19, 54, 20, 63, 44, 63, 66,\n",
       "        65, 63, 62, 64, 56, 36, 58, 65, 63, 63, 64, 62, 60, 55, 63, 53, 63,\n",
       "        62, 65, 63, 28, 60, 12, 21, 63, 61, 64, 62, 65, 65, 64, 63, 65, 62,\n",
       "        64, 62, 13, 64, 63, 40, 47, 65, 58, 64, 52, 63, 62, 61, 65, 64,  1,\n",
       "        62, 59, 61, 65, 66, 62, 60, 63, 61, 62, 65, 62, 62, 62, 64, 65, 63,\n",
       "        64, 24, 16, 62,  9, 21, 62, 65, 62, 66, 65, 63, 38,  7, 60, 63, 65,\n",
       "        14, 60, 65, 65, 62, 63, 61, 60, 64, 65, 46, 63, 62, 63, 62, 63, 58,\n",
       "        64, 63,  9, 59, 58, 57, 63, 65, 61]),\n",
       " 'seq': array([[25, 14, 13, ...,  0,  0,  0],\n",
       "        [25, 14, 13, ...,  0,  0,  0],\n",
       "        [44,  7,  4, ...,  4,  0,  0],\n",
       "        ...,\n",
       "        [53,  3, 26, ...,  0,  0,  0],\n",
       "        [25, 14, 13, ...,  8,  7,  0],\n",
       "        [25, 14, 13, ...,  0,  0,  0]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(next_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self,inputs):\n",
    "        sequence =  inputs['seq']\n",
    "        lengths = inputs['length']\n",
    "        book_id = inputs['book_id']\n",
    "        self.lr = tf.placeholder(shape=None,dtype=tf.float32)\n",
    "        \n",
    "        \n",
    "        emb_vec = tf.get_variable(\"emb\",dtype=tf.float32,shape=[74,32])\n",
    "        emb_source = tf.nn.embedding_lookup(emb_vec,sequence)\n",
    "        \n",
    "        \n",
    "        cell = tf.nn.rnn_cell.GRUCell(128)\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell,emb_source,dtype=tf.float32,sequence_length=lengths)\n",
    "        \n",
    "        book_logits =  tf.contrib.layers.fully_connected(state,num_outputs=64,activation_fn=tf.tanh)\n",
    "        book_logits =  tf.contrib.layers.fully_connected(state,num_outputs=215,activation_fn=None)\n",
    "        \n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(book_id,book_logits)\n",
    "        self.loss = tf.reduce_mean(loss)\n",
    "        opt = tf.train.AdamOptimizer(self.lr)\n",
    "        self.train = opt.minimize(self.loss)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = Model(next_item)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3803573"
     ]
    }
   ],
   "source": [
    "num =1\n",
    "import sys\n",
    "while True:\n",
    "    try:\n",
    "        _,loss = sess.run([M.train,M.loss],feed_dict={M.lr:0.0001})\n",
    "        if num %30==0:\n",
    "            clear_output()\n",
    "        num+=1\n",
    "        sys.stdout.write(\"\\r\" + str(loss))\n",
    "        sys.stdout.flush()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
